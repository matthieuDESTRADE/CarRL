{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "from environment import TorchLikeCarEnv, CarEnv\n",
    "%aimport environment\n",
    "from policy import QNetwork, recover_flattened\n",
    "%aimport policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Evotorch RL example](https://github.com/nnaisense/evotorch?tab=readme-ov-file#a-reinforcement-learning-example)\n",
    "In a GymNE problem, the solver tries to maximize the total reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from evotorch.algorithms import CMAES, CEM\n",
    "from evotorch.logging import StdOutLogger, PicklingLogger\n",
    "from evotorch.neuroevolution import GymNE\n",
    "\n",
    "def gymNE_env(*args, **kwargs): return TorchLikeCarEnv(10, False, evaluation=False, *args, **kwargs)\n",
    "action_dim = gymNE_env().action_space.n\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-10 18:55:02] INFO     <11760> evotorch.core: Instance of `GymNE` (id:1674949213840) -- The `dtype` for the problem's decision variables is set as torch.float32\n",
      "[2024-03-10 18:55:02] INFO     <11760> evotorch.core: Instance of `GymNE` (id:1674949213840) -- `eval_dtype` (the dtype of the fitnesses and evaluation data) is set as torch.float32\n",
      "[2024-03-10 18:55:02] INFO     <11760> evotorch.core: Instance of `GymNE` (id:1674949213840) -- The `device` of the problem is set as cpu\n",
      "[2024-03-10 18:55:02] INFO     <11760> evotorch.core: Instance of `GymNE` (id:1674949213840) -- The number of actors that will be allocated for parallelized evaluation is 0\n",
      "                   iter : 1\n",
      "              mean_eval : 3.5604984760284424\n",
      "          pop_best_eval : 19.455232620239258\n",
      "            median_eval : 4.084464073181152\n",
      "              best_eval : 19.455232620239258\n",
      "             worst_eval : -8.768762588500977\n",
      "total_interaction_count : 625\n",
      "    total_episode_count : 15\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m PicklingLogger(searcher, interval\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, directory\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEA_results\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[39m# Run the algorithm for the specified amount of generations\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m searcher\u001b[39m.\u001b[39;49mrun(\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\bcbav\\.vscode\\environnement_ipython\\INF581\\car_project\\Projet\\.venv\\Lib\\site-packages\\evotorch\\algorithms\\searchalgorithm.py:425\u001b[0m, in \u001b[0;36mSearchAlgorithm.run\u001b[1;34m(self, num_generations, reset_first_step_datetime)\u001b[0m\n\u001b[0;32m    422\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_first_step_datetime()\n\u001b[0;32m    424\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mint\u001b[39m(num_generations)):\n\u001b[1;32m--> 425\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m    427\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_end_of_run_hook) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    428\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_end_of_run_hook(\u001b[39mdict\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus))\n",
      "File \u001b[1;32mc:\\Users\\bcbav\\.vscode\\environnement_ipython\\INF581\\car_project\\Projet\\.venv\\Lib\\site-packages\\evotorch\\algorithms\\searchalgorithm.py:390\u001b[0m, in \u001b[0;36mSearchAlgorithm.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_first_step_datetime \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_first_step_datetime \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n\u001b[1;32m--> 390\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step()\n\u001b[0;32m    391\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    392\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_status({\u001b[39m\"\u001b[39m\u001b[39miter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_count})\n",
      "File \u001b[1;32mc:\\Users\\bcbav\\.vscode\\environnement_ipython\\INF581\\car_project\\Projet\\.venv\\Lib\\site-packages\\evotorch\\algorithms\\distributed\\gaussian.py:367\u001b[0m, in \u001b[0;36mGaussianSearchAlgorithm._step_non_distributed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    363\u001b[0m gradients \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_distribution\u001b[39m.\u001b[39mcompute_gradients(\n\u001b[0;32m    364\u001b[0m     samples, fitnesses, objective_sense\u001b[39m=\u001b[39mobj_sense, ranking_method\u001b[39m=\u001b[39mranking_method\n\u001b[0;32m    365\u001b[0m )\n\u001b[0;32m    366\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_distribution(gradients)\n\u001b[1;32m--> 367\u001b[0m fill_and_eval_pop()\n",
      "File \u001b[1;32mc:\\Users\\bcbav\\.vscode\\environnement_ipython\\INF581\\car_project\\Projet\\.venv\\Lib\\site-packages\\evotorch\\algorithms\\distributed\\gaussian.py:295\u001b[0m, in \u001b[0;36mGaussianSearchAlgorithm._step_non_distributed.<locals>.fill_and_eval_pop\u001b[1;34m()\u001b[0m\n\u001b[0;32m    292\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_distribution\u001b[39m.\u001b[39msample(out\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_population\u001b[39m.\u001b[39maccess_values(), generator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproblem)\n\u001b[0;32m    294\u001b[0m     \u001b[39m# Finally, here, the solutions are evaluated.\u001b[39;00m\n\u001b[1;32m--> 295\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproblem\u001b[39m.\u001b[39;49mevaluate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_population)\n\u001b[0;32m    296\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    297\u001b[0m     \u001b[39m# If num_interactions is not None, then this means that we have a threshold for the number\u001b[39;00m\n\u001b[0;32m    298\u001b[0m     \u001b[39m# of simulator interactions to reach before declaring the phase of sampling complete.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[39m# Therefore, to properly count the simulator interactions we made during this generation, we need\u001b[39;00m\n\u001b[0;32m    304\u001b[0m     \u001b[39m# to get the interaction count before starting our sampling and evaluation operations.\u001b[39;00m\n\u001b[0;32m    305\u001b[0m     first_num_interactions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproblem\u001b[39m.\u001b[39mstatus\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtotal_interaction_count\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bcbav\\.vscode\\environnement_ipython\\INF581\\car_project\\Projet\\.venv\\Lib\\site-packages\\evotorch\\core.py:2548\u001b[0m, in \u001b[0;36mProblem.evaluate\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   2544\u001b[0m must_sync_after \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sync_before()\n\u001b[0;32m   2546\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_preparations()\n\u001b[1;32m-> 2548\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluate_all(batch)\n\u001b[0;32m   2550\u001b[0m \u001b[39mif\u001b[39;00m must_sync_after:\n\u001b[0;32m   2551\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sync_after()\n",
      "File \u001b[1;32mc:\\Users\\bcbav\\.vscode\\environnement_ipython\\INF581\\car_project\\Projet\\.venv\\Lib\\site-packages\\evotorch\\core.py:2566\u001b[0m, in \u001b[0;36mProblem._evaluate_all\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m   2564\u001b[0m fitness_device \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device_of_fitness_function()\n\u001b[0;32m   2565\u001b[0m \u001b[39mif\u001b[39;00m fitness_device \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 2566\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluate_batch(batch)\n\u001b[0;32m   2567\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2568\u001b[0m     original_device \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mdevice\n",
      "File \u001b[1;32mc:\\Users\\bcbav\\.vscode\\environnement_ipython\\INF581\\car_project\\Projet\\.venv\\Lib\\site-packages\\evotorch\\core.py:2600\u001b[0m, in \u001b[0;36mProblem._evaluate_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m   2598\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2599\u001b[0m     \u001b[39mfor\u001b[39;00m sln \u001b[39min\u001b[39;00m batch:\n\u001b[1;32m-> 2600\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluate(sln)\n",
      "File \u001b[1;32mc:\\Users\\bcbav\\.vscode\\environnement_ipython\\INF581\\car_project\\Projet\\.venv\\Lib\\site-packages\\evotorch\\neuroevolution\\neproblem.py:424\u001b[0m, in \u001b[0;36mNEProblem._evaluate\u001b[1;34m(self, solution)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    422\u001b[0m     evaluator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_network_eval_func\n\u001b[1;32m--> 424\u001b[0m fitnesses \u001b[39m=\u001b[39m evaluator(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparameterize_net(parameters))\n\u001b[0;32m    426\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fitnesses, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    427\u001b[0m     solution\u001b[39m.\u001b[39mset_evals(\u001b[39m*\u001b[39mfitnesses)\n",
      "File \u001b[1;32mc:\\Users\\bcbav\\.vscode\\environnement_ipython\\INF581\\car_project\\Projet\\.venv\\Lib\\site-packages\\evotorch\\neuroevolution\\gymne.py:637\u001b[0m, in \u001b[0;36mGymNE._evaluate_network\u001b[1;34m(self, policy)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_evaluate_network\u001b[39m(\u001b[39mself\u001b[39m, policy: nn\u001b[39m.\u001b[39mModule) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[\u001b[39mfloat\u001b[39m, torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 637\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(\n\u001b[0;32m    638\u001b[0m         policy,\n\u001b[0;32m    639\u001b[0m         update_stats\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    640\u001b[0m         visualize\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    641\u001b[0m         num_episodes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_episodes,\n\u001b[0;32m    642\u001b[0m         decrease_rewards_by\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decrease_rewards_by,\n\u001b[0;32m    643\u001b[0m     )\n\u001b[0;32m    644\u001b[0m     \u001b[39mreturn\u001b[39;00m result[\u001b[39m\"\u001b[39m\u001b[39mcumulative_reward\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\bcbav\\.vscode\\environnement_ipython\\INF581\\car_project\\Projet\\.venv\\Lib\\site-packages\\evotorch\\neuroevolution\\gymne.py:462\u001b[0m, in \u001b[0;36mGymNE.run\u001b[1;34m(self, policy, update_stats, visualize, num_episodes, decrease_rewards_by)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     policy\u001b[39m.\u001b[39meval()\n\u001b[1;32m--> 462\u001b[0m     episode_results \u001b[39m=\u001b[39m [\n\u001b[0;32m    463\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_rollout(\n\u001b[0;32m    464\u001b[0m             policy\u001b[39m=\u001b[39;49mpolicy,\n\u001b[0;32m    465\u001b[0m             update_stats\u001b[39m=\u001b[39;49mupdate_stats,\n\u001b[0;32m    466\u001b[0m             visualize\u001b[39m=\u001b[39;49mvisualize,\n\u001b[0;32m    467\u001b[0m             decrease_rewards_by\u001b[39m=\u001b[39;49mdecrease_rewards_by,\n\u001b[0;32m    468\u001b[0m         )\n\u001b[0;32m    469\u001b[0m         \u001b[39mfor\u001b[39;49;00m _ \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(num_episodes)\n\u001b[0;32m    470\u001b[0m     ]\n\u001b[0;32m    472\u001b[0m     results \u001b[39m=\u001b[39m _accumulate_all_across_dicts(episode_results, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_keys)\n\u001b[0;32m    473\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\Users\\bcbav\\.vscode\\environnement_ipython\\INF581\\car_project\\Projet\\.venv\\Lib\\site-packages\\evotorch\\neuroevolution\\gymne.py:463\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     policy\u001b[39m.\u001b[39meval()\n\u001b[0;32m    462\u001b[0m     episode_results \u001b[39m=\u001b[39m [\n\u001b[1;32m--> 463\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_rollout(\n\u001b[0;32m    464\u001b[0m             policy\u001b[39m=\u001b[39;49mpolicy,\n\u001b[0;32m    465\u001b[0m             update_stats\u001b[39m=\u001b[39;49mupdate_stats,\n\u001b[0;32m    466\u001b[0m             visualize\u001b[39m=\u001b[39;49mvisualize,\n\u001b[0;32m    467\u001b[0m             decrease_rewards_by\u001b[39m=\u001b[39;49mdecrease_rewards_by,\n\u001b[0;32m    468\u001b[0m         )\n\u001b[0;32m    469\u001b[0m         \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_episodes)\n\u001b[0;32m    470\u001b[0m     ]\n\u001b[0;32m    472\u001b[0m     results \u001b[39m=\u001b[39m _accumulate_all_across_dicts(episode_results, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_keys)\n\u001b[0;32m    473\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\Users\\bcbav\\.vscode\\environnement_ipython\\INF581\\car_project\\Projet\\.venv\\Lib\\site-packages\\evotorch\\neuroevolution\\gymne.py:391\u001b[0m, in \u001b[0;36mGymNE._rollout\u001b[1;34m(self, policy, update_stats, visualize, decrease_rewards_by)\u001b[0m\n\u001b[0;32m    388\u001b[0m cumulative_reward \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m    390\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m     observation, raw_reward, done, info \u001b[39m=\u001b[39m take_step_in_env(env, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_use_policy(observation, policy))\n\u001b[0;32m    392\u001b[0m     reward \u001b[39m=\u001b[39m raw_reward \u001b[39m-\u001b[39m decrease_rewards_by\n\u001b[0;32m    393\u001b[0m     t \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\bcbav\\.vscode\\environnement_ipython\\INF581\\car_project\\Projet\\.venv\\Lib\\site-packages\\evotorch\\neuroevolution\\gymne.py:336\u001b[0m, in \u001b[0;36mGymNE._use_policy\u001b[1;34m(self, observation, policy)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_use_policy\u001b[39m(\u001b[39mself\u001b[39m, observation: Iterable, policy: nn\u001b[39m.\u001b[39mModule) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterable:\n\u001b[0;32m    335\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 336\u001b[0m         result \u001b[39m=\u001b[39m policy(torch\u001b[39m.\u001b[39;49mas_tensor(observation, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32, device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m))\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m    337\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_action_noise_stdev \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    338\u001b[0m         result \u001b[39m=\u001b[39m (\n\u001b[0;32m    339\u001b[0m             result\n\u001b[0;32m    340\u001b[0m             \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_gaussian(\u001b[39mlen\u001b[39m(result), center\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m, stdev\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_action_noise_stdev, device\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m    341\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\bcbav\\.vscode\\environnement_ipython\\INF581\\car_project\\Projet\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bcbav\\.vscode\\environnement_ipython\\INF581\\car_project\\Projet\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bcbav\\.vscode\\environnement_ipython\\INF581\\car_project\\Projet\\.venv\\Lib\\site-packages\\evotorch\\neuroevolution\\net\\statefulmodule.py:59\u001b[0m, in \u001b[0;36mStatefulModule.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m     57\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hidden \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m         \u001b[39m# If there is no stored hidden state, then only pass the input tensor to the wrapped module.\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m         out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrapped_module(x)\n\u001b[0;32m     60\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m         \u001b[39m# If there is a hidden state saved from the previous call to this `forward(...)` method, then pass the\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         \u001b[39m# input tensor and this stored hidden state.\u001b[39;00m\n\u001b[0;32m     63\u001b[0m         out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrapped_module(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hidden)\n",
      "File \u001b[1;32mc:\\Users\\bcbav\\.vscode\\environnement_ipython\\INF581\\car_project\\Projet\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bcbav\\.vscode\\environnement_ipython\\INF581\\car_project\\Projet\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bcbav\\.vscode\\environnement_ipython\\INF581\\car_project\\Projet\\policy.py:43\u001b[0m, in \u001b[0;36mQNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     41\u001b[0m     \u001b[39m# Define the forward pass of the CNN\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)\n\u001b[1;32m---> 43\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrelu1(x)\n\u001b[0;32m     44\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool1(x)\n\u001b[0;32m     45\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)\n",
      "File \u001b[1;32mc:\\Users\\bcbav\\.vscode\\environnement_ipython\\INF581\\car_project\\Projet\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bcbav\\.vscode\\environnement_ipython\\INF581\\car_project\\Projet\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bcbav\\.vscode\\environnement_ipython\\INF581\\car_project\\Projet\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:774\u001b[0m, in \u001b[0;36mLeakyReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 774\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mleaky_relu(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnegative_slope, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[1;32mc:\\Users\\bcbav\\.vscode\\environnement_ipython\\INF581\\car_project\\Projet\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:1648\u001b[0m, in \u001b[0;36mleaky_relu\u001b[1;34m(input, negative_slope, inplace)\u001b[0m\n\u001b[0;32m   1646\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mleaky_relu_(\u001b[39minput\u001b[39m, negative_slope)\n\u001b[0;32m   1647\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1648\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mleaky_relu(\u001b[39minput\u001b[39;49m, negative_slope)\n\u001b[0;32m   1649\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "q_network = QNetwork(action_dim).to(device)\n",
    "problem = GymNE(\n",
    "    env=gymNE_env,\n",
    "    network=q_network,\n",
    "    num_episodes=1,\n",
    ")\n",
    "\n",
    "searcher = CEM(\n",
    "    problem,\n",
    "    # The keyword arguments below refer to hyperparameters specific to the\n",
    "    # cross entropy method algorithm. It is recommended to tune these\n",
    "    # hyperparameters according to the problem at hand.\n",
    "    popsize=15,  # population size\n",
    "    parenthood_ratio=0.3,  # better solutions become parents\n",
    "    stdev_init=10.0,  # initial standard deviation of the search distribution\n",
    ")\n",
    "\n",
    "# Instantiate a standard output logger\n",
    "StdOutLogger(searcher)\n",
    "\n",
    "# Optional: Instantiate a logger to pickle and save the results periodically.\n",
    "# In this example, among the saved results will be the center of the search\n",
    "# distribution, since we are using PGPE which is distribution-based.\n",
    "PicklingLogger(searcher, interval=10, directory=\"EA_results\")\n",
    "\n",
    "# Run the algorithm for the specified amount of generations\n",
    "searcher.run(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-10 18:06:06] INFO     <11760> evotorch.core: Instance of `GymNE` (id:1674915931152) -- The `dtype` for the problem's decision variables is set as torch.float32\n",
      "[2024-03-10 18:06:06] INFO     <11760> evotorch.core: Instance of `GymNE` (id:1674915931152) -- `eval_dtype` (the dtype of the fitnesses and evaluation data) is set as torch.float32\n",
      "[2024-03-10 18:06:06] INFO     <11760> evotorch.core: Instance of `GymNE` (id:1674915931152) -- The `device` of the problem is set as cpu\n",
      "[2024-03-10 18:06:06] INFO     <11760> evotorch.core: Instance of `GymNE` (id:1674915931152) -- The number of actors that will be allocated for parallelized evaluation is 0\n",
      "                   iter : 1\n",
      "              mean_eval : 0.04982553794980049\n",
      "          pop_best_eval : 10.844755172729492\n",
      "            median_eval : 3.2594640254974365\n",
      "              best_eval : 10.844755172729492\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 898\n",
      "    total_episode_count : 20\n",
      "\n",
      "                   iter : 2\n",
      "              mean_eval : -0.12056197971105576\n",
      "          pop_best_eval : 8.998655319213867\n",
      "            median_eval : -1.8864582777023315\n",
      "              best_eval : 10.844755172729492\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 1724\n",
      "    total_episode_count : 40\n",
      "\n",
      "                   iter : 3\n",
      "              mean_eval : 5.3089447021484375\n",
      "          pop_best_eval : 26.533119201660156\n",
      "            median_eval : 5.574542045593262\n",
      "              best_eval : 26.533119201660156\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 2643\n",
      "    total_episode_count : 60\n",
      "\n",
      "                   iter : 4\n",
      "              mean_eval : -0.07128741592168808\n",
      "          pop_best_eval : 12.89936637878418\n",
      "            median_eval : -2.4105753898620605\n",
      "              best_eval : 26.533119201660156\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 3481\n",
      "    total_episode_count : 80\n",
      "\n",
      "                   iter : 5\n",
      "              mean_eval : 5.829281806945801\n",
      "          pop_best_eval : 55.566280364990234\n",
      "            median_eval : 4.384042263031006\n",
      "              best_eval : 55.566280364990234\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 4528\n",
      "    total_episode_count : 100\n",
      "\n",
      "                   iter : 6\n",
      "              mean_eval : 3.759063720703125\n",
      "          pop_best_eval : 11.128018379211426\n",
      "            median_eval : 6.028505802154541\n",
      "              best_eval : 55.566280364990234\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 5437\n",
      "    total_episode_count : 120\n",
      "\n",
      "                   iter : 7\n",
      "              mean_eval : 9.76563835144043\n",
      "          pop_best_eval : 69.3368148803711\n",
      "            median_eval : 4.432125091552734\n",
      "              best_eval : 69.3368148803711\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 6678\n",
      "    total_episode_count : 140\n",
      "\n",
      "                   iter : 8\n",
      "              mean_eval : 9.20088005065918\n",
      "          pop_best_eval : 105.9648666381836\n",
      "            median_eval : 4.593688488006592\n",
      "              best_eval : 105.9648666381836\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 7800\n",
      "    total_episode_count : 160\n",
      "\n",
      "                   iter : 9\n",
      "              mean_eval : 7.476925849914551\n",
      "          pop_best_eval : 70.55776977539062\n",
      "            median_eval : 6.661575794219971\n",
      "              best_eval : 105.9648666381836\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 9193\n",
      "    total_episode_count : 180\n",
      "\n",
      "                   iter : 10\n",
      "              mean_eval : 10.87798023223877\n",
      "          pop_best_eval : 70.65438842773438\n",
      "            median_eval : 4.488537311553955\n",
      "              best_eval : 105.9648666381836\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 10626\n",
      "    total_episode_count : 200\n",
      "\n",
      "                   iter : 11\n",
      "              mean_eval : 6.499405860900879\n",
      "          pop_best_eval : 28.238906860351562\n",
      "            median_eval : 6.973263740539551\n",
      "              best_eval : 105.9648666381836\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 11680\n",
      "    total_episode_count : 220\n",
      "\n",
      "                   iter : 12\n",
      "              mean_eval : 10.581979751586914\n",
      "          pop_best_eval : 85.50794219970703\n",
      "            median_eval : 5.153702259063721\n",
      "              best_eval : 105.9648666381836\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 12942\n",
      "    total_episode_count : 240\n",
      "\n",
      "                   iter : 13\n",
      "              mean_eval : 11.06602668762207\n",
      "          pop_best_eval : 67.7269287109375\n",
      "            median_eval : 6.010682582855225\n",
      "              best_eval : 105.9648666381836\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 14102\n",
      "    total_episode_count : 260\n",
      "\n",
      "                   iter : 14\n",
      "              mean_eval : 6.098099708557129\n",
      "          pop_best_eval : 23.740924835205078\n",
      "            median_eval : 6.290831089019775\n",
      "              best_eval : 105.9648666381836\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 15100\n",
      "    total_episode_count : 280\n",
      "\n",
      "                   iter : 15\n",
      "              mean_eval : 12.596197128295898\n",
      "          pop_best_eval : 60.621280670166016\n",
      "            median_eval : 6.5009989738464355\n",
      "              best_eval : 105.9648666381836\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 16546\n",
      "    total_episode_count : 300\n",
      "\n",
      "                   iter : 16\n",
      "              mean_eval : 16.090961456298828\n",
      "          pop_best_eval : 92.76439666748047\n",
      "            median_eval : 7.757014274597168\n",
      "              best_eval : 105.9648666381836\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 18032\n",
      "    total_episode_count : 320\n",
      "\n",
      "                   iter : 17\n",
      "              mean_eval : 11.638312339782715\n",
      "          pop_best_eval : 43.21437072753906\n",
      "            median_eval : 7.851982116699219\n",
      "              best_eval : 105.9648666381836\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 19331\n",
      "    total_episode_count : 340\n",
      "\n",
      "                   iter : 18\n",
      "              mean_eval : 11.80453109741211\n",
      "          pop_best_eval : 63.137786865234375\n",
      "            median_eval : 8.50796127319336\n",
      "              best_eval : 105.9648666381836\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 20572\n",
      "    total_episode_count : 360\n",
      "\n",
      "                   iter : 19\n",
      "              mean_eval : 15.115518569946289\n",
      "          pop_best_eval : 66.95719146728516\n",
      "            median_eval : 9.138301849365234\n",
      "              best_eval : 105.9648666381836\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 21950\n",
      "    total_episode_count : 380\n",
      "\n",
      "                   iter : 20\n",
      "              mean_eval : 14.661747932434082\n",
      "          pop_best_eval : 70.5242919921875\n",
      "            median_eval : 8.7377290725708\n",
      "              best_eval : 105.9648666381836\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 23488\n",
      "    total_episode_count : 400\n",
      "\n",
      "                   iter : 21\n",
      "              mean_eval : 20.882389068603516\n",
      "          pop_best_eval : 82.42039489746094\n",
      "            median_eval : 8.088264465332031\n",
      "              best_eval : 105.9648666381836\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 25166\n",
      "    total_episode_count : 420\n",
      "\n",
      "                   iter : 22\n",
      "              mean_eval : 31.236730575561523\n",
      "          pop_best_eval : 97.71922302246094\n",
      "            median_eval : 22.975717544555664\n",
      "              best_eval : 105.9648666381836\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 27575\n",
      "    total_episode_count : 440\n",
      "\n",
      "                   iter : 23\n",
      "              mean_eval : 14.414471626281738\n",
      "          pop_best_eval : 43.597660064697266\n",
      "            median_eval : 7.569087505340576\n",
      "              best_eval : 105.9648666381836\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 28906\n",
      "    total_episode_count : 460\n",
      "\n",
      "                   iter : 24\n",
      "              mean_eval : 16.76059913635254\n",
      "          pop_best_eval : 78.4247055053711\n",
      "            median_eval : 6.241520881652832\n",
      "              best_eval : 105.9648666381836\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 30378\n",
      "    total_episode_count : 480\n",
      "\n",
      "                   iter : 25\n",
      "              mean_eval : 21.327730178833008\n",
      "          pop_best_eval : 69.11101531982422\n",
      "            median_eval : 13.117871284484863\n",
      "              best_eval : 105.9648666381836\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 32174\n",
      "    total_episode_count : 500\n",
      "\n",
      "                   iter : 26\n",
      "              mean_eval : 12.058351516723633\n",
      "          pop_best_eval : 41.32778549194336\n",
      "            median_eval : 6.905976295471191\n",
      "              best_eval : 105.9648666381836\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 33427\n",
      "    total_episode_count : 520\n",
      "\n",
      "                   iter : 27\n",
      "              mean_eval : 21.504268646240234\n",
      "          pop_best_eval : 107.75101470947266\n",
      "            median_eval : 11.008405685424805\n",
      "              best_eval : 107.75101470947266\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 35191\n",
      "    total_episode_count : 540\n",
      "\n",
      "                   iter : 28\n",
      "              mean_eval : 15.209320068359375\n",
      "          pop_best_eval : 93.16949462890625\n",
      "            median_eval : 9.425811767578125\n",
      "              best_eval : 107.75101470947266\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 36600\n",
      "    total_episode_count : 560\n",
      "\n",
      "                   iter : 29\n",
      "              mean_eval : 23.09023666381836\n",
      "          pop_best_eval : 107.38607025146484\n",
      "            median_eval : 7.173940181732178\n",
      "              best_eval : 107.75101470947266\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 38484\n",
      "    total_episode_count : 580\n",
      "\n",
      "                   iter : 30\n",
      "              mean_eval : 20.017244338989258\n",
      "          pop_best_eval : 54.84218215942383\n",
      "            median_eval : 19.07754898071289\n",
      "              best_eval : 107.75101470947266\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 40347\n",
      "    total_episode_count : 600\n",
      "\n",
      "                   iter : 31\n",
      "              mean_eval : 21.817636489868164\n",
      "          pop_best_eval : 106.94405364990234\n",
      "            median_eval : 9.391530990600586\n",
      "              best_eval : 107.75101470947266\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 42224\n",
      "    total_episode_count : 620\n",
      "\n",
      "                   iter : 32\n",
      "              mean_eval : 23.58711814880371\n",
      "          pop_best_eval : 74.18291473388672\n",
      "            median_eval : 11.737768173217773\n",
      "              best_eval : 107.75101470947266\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 44123\n",
      "    total_episode_count : 640\n",
      "\n",
      "                   iter : 33\n",
      "              mean_eval : 13.960929870605469\n",
      "          pop_best_eval : 41.4915885925293\n",
      "            median_eval : 11.316167831420898\n",
      "              best_eval : 107.75101470947266\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 45488\n",
      "    total_episode_count : 660\n",
      "\n",
      "                   iter : 34\n",
      "              mean_eval : 22.238019943237305\n",
      "          pop_best_eval : 107.90327453613281\n",
      "            median_eval : 7.8746232986450195\n",
      "              best_eval : 107.90327453613281\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 47274\n",
      "    total_episode_count : 680\n",
      "\n",
      "                   iter : 35\n",
      "              mean_eval : 22.01898193359375\n",
      "          pop_best_eval : 107.84361267089844\n",
      "            median_eval : 6.901859760284424\n",
      "              best_eval : 107.90327453613281\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 48987\n",
      "    total_episode_count : 700\n",
      "\n",
      "                   iter : 36\n",
      "              mean_eval : 14.571922302246094\n",
      "          pop_best_eval : 91.16405487060547\n",
      "            median_eval : 8.114922523498535\n",
      "              best_eval : 107.90327453613281\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 50375\n",
      "    total_episode_count : 720\n",
      "\n",
      "                   iter : 37\n",
      "              mean_eval : 12.242144584655762\n",
      "          pop_best_eval : 36.54985046386719\n",
      "            median_eval : 6.958395957946777\n",
      "              best_eval : 107.90327453613281\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 51593\n",
      "    total_episode_count : 740\n",
      "\n",
      "                   iter : 38\n",
      "              mean_eval : 23.636762619018555\n",
      "          pop_best_eval : 107.37393188476562\n",
      "            median_eval : 9.987800598144531\n",
      "              best_eval : 107.90327453613281\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 53424\n",
      "    total_episode_count : 760\n",
      "\n",
      "                   iter : 39\n",
      "              mean_eval : 13.194132804870605\n",
      "          pop_best_eval : 72.00494384765625\n",
      "            median_eval : 9.068106651306152\n",
      "              best_eval : 107.90327453613281\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 54693\n",
      "    total_episode_count : 780\n",
      "\n",
      "                   iter : 40\n",
      "              mean_eval : 20.48598289489746\n",
      "          pop_best_eval : 85.77851104736328\n",
      "            median_eval : 9.193832397460938\n",
      "              best_eval : 107.90327453613281\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 56350\n",
      "    total_episode_count : 800\n",
      "\n",
      "                   iter : 41\n",
      "              mean_eval : 15.303875923156738\n",
      "          pop_best_eval : 74.14271545410156\n",
      "            median_eval : 9.010546684265137\n",
      "              best_eval : 107.90327453613281\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 57789\n",
      "    total_episode_count : 820\n",
      "\n",
      "                   iter : 42\n",
      "              mean_eval : 12.892555236816406\n",
      "          pop_best_eval : 48.43787384033203\n",
      "            median_eval : 7.068347454071045\n",
      "              best_eval : 107.90327453613281\n",
      "             worst_eval : -44.87783432006836\n",
      "total_interaction_count : 59188\n",
      "    total_episode_count : 840\n",
      "\n",
      "                   iter : 43\n",
      "              mean_eval : 10.538838386535645\n",
      "          pop_best_eval : 83.79940795898438\n",
      "            median_eval : 8.71557903289795\n",
      "              best_eval : 107.90327453613281\n",
      "             worst_eval : -95.84756469726562\n",
      "total_interaction_count : 60820\n",
      "    total_episode_count : 860\n",
      "\n",
      "                   iter : 44\n",
      "              mean_eval : 9.372861862182617\n",
      "          pop_best_eval : 21.325185775756836\n",
      "            median_eval : 8.003110885620117\n",
      "              best_eval : 107.90327453613281\n",
      "             worst_eval : -95.84756469726562\n",
      "total_interaction_count : 61876\n",
      "    total_episode_count : 880\n",
      "\n",
      "                   iter : 45\n",
      "              mean_eval : 12.529266357421875\n",
      "          pop_best_eval : 27.797428131103516\n",
      "            median_eval : 8.66664981842041\n",
      "              best_eval : 107.90327453613281\n",
      "             worst_eval : -95.84756469726562\n",
      "total_interaction_count : 63115\n",
      "    total_episode_count : 900\n",
      "\n",
      "                   iter : 46\n",
      "              mean_eval : 24.022985458374023\n",
      "          pop_best_eval : 107.75906372070312\n",
      "            median_eval : 8.358230590820312\n",
      "              best_eval : 107.90327453613281\n",
      "             worst_eval : -95.84756469726562\n",
      "total_interaction_count : 64985\n",
      "    total_episode_count : 920\n",
      "\n",
      "                   iter : 47\n",
      "              mean_eval : 16.908817291259766\n",
      "          pop_best_eval : 81.45830535888672\n",
      "            median_eval : 10.644381523132324\n",
      "              best_eval : 107.90327453613281\n",
      "             worst_eval : -95.84756469726562\n",
      "total_interaction_count : 66682\n",
      "    total_episode_count : 940\n",
      "\n",
      "                   iter : 48\n",
      "              mean_eval : 16.03277015686035\n",
      "          pop_best_eval : 55.0262336730957\n",
      "            median_eval : 11.211163520812988\n",
      "              best_eval : 107.90327453613281\n",
      "             worst_eval : -95.84756469726562\n",
      "total_interaction_count : 68228\n",
      "    total_episode_count : 960\n",
      "\n",
      "                   iter : 49\n",
      "              mean_eval : 24.376977920532227\n",
      "          pop_best_eval : 84.41490173339844\n",
      "            median_eval : 11.240357398986816\n",
      "              best_eval : 107.90327453613281\n",
      "             worst_eval : -95.84756469726562\n",
      "total_interaction_count : 70095\n",
      "    total_episode_count : 980\n",
      "\n",
      "                   iter : 50\n",
      "              mean_eval : 10.656930923461914\n",
      "          pop_best_eval : 63.060298919677734\n",
      "            median_eval : 8.072793006896973\n",
      "              best_eval : 107.90327453613281\n",
      "             worst_eval : -95.84756469726562\n",
      "total_interaction_count : 71210\n",
      "    total_episode_count : 1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from evotorch.algorithms import SNES\n",
    "q_network = QNetwork(action_dim).to(device)\n",
    "problem = GymNE(\n",
    "    env=gymNE_env,\n",
    "    network=q_network,\n",
    "    num_episodes=1,\n",
    "    action_noise_stdev=0.1,\n",
    ")\n",
    "searcher = SNES(problem, stdev_init=5, popsize=20, popsize_max=40)\n",
    "StdOutLogger(searcher)\n",
    "searcher.run(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = searcher.status[\"pop_best\"]\n",
    "recover_flattened(best, q_network)\n",
    "torch.save(q_network, \"models/CEM.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.44203730871672\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"models/CEM.pth\", map_location=device)\n",
    "env = CarEnv(300, display=True, evaluation=False, draw_central_line=True)\n",
    "\n",
    "obs = env.reset()\n",
    "score = 0\n",
    "\n",
    "while True:\n",
    "    obstensor = CarEnv.obs2tensor(obs, device)\n",
    "    action = q_network(obstensor).argmax(dim=1)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    score += reward\n",
    "    if done:\n",
    "        break\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
